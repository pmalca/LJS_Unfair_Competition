{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "import time \n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "df = pd.read_pickle(r\"corpus_ccd_sumilla_2009_2024.pkl\")\n",
    "df = df.drop(columns=['Text'])\n",
    "df_perpage = pd.read_pickle(r\"corpus_ccd_per_page.pkl\")\n",
    "df_perpage['ID'] = df_perpage['ID'].str.replace('.pdf', '', regex=False)\n",
    "df_perpage = df_perpage.merge(df, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the text for the embeddings\n",
    "\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Resumen_GPT'] + ' ' + df_perpage['Text']\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].str.replace(r'[\\n\\r\\t]', ' ', regex=True).str.strip()\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].str.replace(r'\\s+', ' ', regex=True)\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].str.replace(r'\\xa0', ' ', regex=True)\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].str.lower()\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].str.replace('*', '', regex=False)\n",
    "df_perpage['Text_for_embeddings'] = df_perpage['Text_for_embeddings'].astype(str)\n",
    "\n",
    "summaries = df_perpage['Text_for_embeddings'].tolist()  # Extract summaries for the embeddings\n",
    "metadata = df_perpage[['Número de Resolución', 'Fecha de Resolución', 'Enlace']]   # Extract other metadata (e.g., ID, other columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining the function that generates embeddings for a list of input texts using the OpenAI API.\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    '''\n",
    "    Generates embeddings for a list of input texts using the OpenAI API, handling token limits, rate limits, and connection timeouts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list of str\n",
    "        A list of textual data (documents, sentences, etc.) to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.array\n",
    "        A NumPy array containing the embeddings for each input text.\n",
    "\n",
    "    Functionality:\n",
    "    --------------\n",
    "    1. Splits input texts into smaller chunks based on the specified token limit (`max_tokens`).\n",
    "    2. Sends each chunk to the OpenAI API to get embeddings, using the model specified (`text-embedding-3-small`).\n",
    "    3. Handles potential errors:\n",
    "        - `RateLimitError`: If the API rate limit is exceeded, waits for a specified time (`rate_limit_wait_time`) and retries.\n",
    "        - `APIConnectionError`: If there is a connection issue, waits for a specified time (`timeout_wait_time`) and retries.\n",
    "        - Other exceptions are caught and raised.\n",
    "    4. Combines the embeddings from all chunks into a single NumPy array and returns it.\n",
    "\n",
    "    Error Handling:\n",
    "    ---------------\n",
    "    - RateLimitError: Prints a message and waits before retrying.\n",
    "    - APIConnectionError: Prints a message and waits before retrying.\n",
    "    - Any other error is raised after printing the error message.\n",
    "    '''\n",
    "\n",
    "    max_tokens = 8192  # Define max tokens per request\n",
    "    rate_limit_wait_time = 60  # Time to wait in seconds if rate limit is hit\n",
    "    timeout_wait_time = 20  # Time to wait in seconds if a timeout occurs\n",
    "\n",
    "\n",
    "    def split_texts(texts, max_tokens):\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        for text in texts:\n",
    "            text_length = len(text.split())  # Word count\n",
    "            if current_length + text_length > max_tokens:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [text]\n",
    "                current_length = text_length\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_length += text_length\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        return chunks\n",
    "    \n",
    "\n",
    "    def get_embeddings_for_chunk(chunk):\n",
    "        while True:\n",
    "            try:\n",
    "                response = openai.Embedding.create(\n",
    "                    model=\"text-embedding-3-small\",  # Choose appropriate model\n",
    "                    input=chunk\n",
    "                )\n",
    "                return [data['embedding'] for data in response['data']]\n",
    "            except openai.error.RateLimitError:\n",
    "                print(f\"Rate limit exceeded. Waiting for {rate_limit_wait_time} seconds...\")\n",
    "                time.sleep(rate_limit_wait_time)\n",
    "            except openai.error.APIConnectionError:\n",
    "                print(f\"Connection timed out. Waiting for {timeout_wait_time} seconds...\")\n",
    "                time.sleep(timeout_wait_time)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "    # Split texts into chunks and get embeddings\n",
    "    chunks = split_texts(texts, max_tokens)\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embeddings.extend(get_embeddings_for_chunk(chunk))\n",
    "\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = 'corpus_ccd_embeddings_full.pkl' # Save your embeddings here\n",
    "\n",
    "print(\"Computing embeddings...\")\n",
    "summary_embeddings = get_embeddings(summaries)\n",
    "\n",
    "    # Save the metadata, summaries, and embeddings using pickle\n",
    "with open(embeddings_path, 'wb') as file:\n",
    "    pickle.dump({\n",
    "            'metadata': metadata,  # Save the metadata (e.g., IDs, other columns)\n",
    "            'summaries': summaries,\n",
    "            'summary_embeddings': summary_embeddings\n",
    "        }, file)\n",
    "    print(\"Embeddings, summaries, and metadata saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
